{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\pl_bolts\\__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\pl_bolts\\__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightning\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from model_fusion.config import BASE_DATA_DIR, CHECKPOINT_DIR\n",
    "from model_fusion.datasets import DataModuleType\n",
    "from model_fusion.models import ModelType\n",
    "from model_fusion.models.lightning import BaseModel \n",
    "from Experiments import lmc_experiment\n",
    "from model_fusion import lmc_utils\n",
    "from Experiments import baselines_experiment\n",
    "from Experiments import otfusion_experiment\n",
    "from Experiments import pyhessian_experiment\n",
    "from model_fusion.train import setup_training, setup_testing\n",
    "\n",
    "\n",
    "# set seed for numpy based calculations\n",
    "NUMPY_SEED = 100\n",
    "np.random.seed(NUMPY_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Loading models -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.025, 'momentum': 0.9, 'data_seed': 42, 'optimizer': 'sgd', 'max_epochs': 200, 'min_epochs': 50, 'model_seed': 42, 'model_type': 'ModelType.VGG11', 'loss_module': 'CrossEntropyLoss', 'lr_scheduler': 'plateau', 'weight_decay': 0.0001, 'model_hparams': {'bias': False, 'num_classes': 10, 'num_channels': 3}, 'early_stopping': True, 'datamodule_type': 'DataModuleType.CIFAR10', 'lr_decay_factor': 0.1, 'lightning_params': {'lr': 0.025, 'momentum': 0.9, 'data_seed': 42, 'optimizer': 'sgd', 'model_seed': 42, 'lr_scheduler': 'plateau', 'weight_decay': 0.0001, 'lr_decay_factor': 0.1, 'lr_monitor_metric': 'val_loss'}, 'lr_monitor_metric': 'val_loss', 'datamodule_hparams': {'seed': 42, 'data_dir': 'data', 'batch_size': 32, 'data_augmentation': True}, 'model_hparams/bias': False, 'model_hparams/num_classes': 10, 'model_hparams/num_channels': 3}\n",
      "{'seed': 42, 'data_dir': 'data', 'batch_size': 32, 'data_augmentation': False}\n",
      "{'bias': False, 'num_classes': 10, 'num_channels': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mframbelli\u001b[0m (\u001b[33mmodel-fusion\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\Notebooks\\wandb\\run-20240112_121623-rnkmrxv1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/model-fusion/model-fusion/runs/rnkmrxv1' target=\"_blank\">trim-feather-207</a></strong> to <a href='https://wandb.ai/model-fusion/model-fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/model-fusion/model-fusion' target=\"_blank\">https://wandb.ai/model-fusion/model-fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/model-fusion/model-fusion/runs/rnkmrxv1' target=\"_blank\">https://wandb.ai/model-fusion/model-fusion/runs/rnkmrxv1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-wil30lcb:best_k, 982.65MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:2.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-6v6im8ni:best_k, 982.65MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:41.8\n"
     ]
    }
   ],
   "source": [
    "print(\"------- Loading models -------\")\n",
    "\n",
    "# select wandb run names\n",
    "runA = 'wil30lcb'\n",
    "runB = '6v6im8ni'#same init\n",
    "\n",
    "api = wandb.Api()\n",
    "run = api.run(f'model-fusion/Model Fusion/{runA}')\n",
    "\n",
    "print(run.config)\n",
    "\n",
    "batch_size = run.config['datamodule_hparams'].get('batch_size')\n",
    "\n",
    "datamodule_type_str = run.config['datamodule_type'].split('.')[1].lower()\n",
    "datamodule_type = DataModuleType(datamodule_type_str)\n",
    "datamodule_hparams = run.config['datamodule_hparams']\n",
    "datamodule_hparams['data_augmentation'] = False\n",
    "\n",
    "model_type_str = run.config['model_type'].split('.')[1].lower()\n",
    "model_type = ModelType(model_type_str)\n",
    "\n",
    "model_hparams = run.config['model_hparams']\n",
    "\n",
    "print(datamodule_hparams)\n",
    "print(model_hparams)\n",
    "\n",
    "checkpointA = f'model-fusion/Model Fusion/model-{runA}:best_k'\n",
    "checkpointB = f'model-fusion/Model Fusion/model-{runB}:best_k'\n",
    "\n",
    "run = wandb.init()\n",
    "\n",
    "artifact = run.use_artifact(checkpointA, type='model')\n",
    "artifact_dir = artifact.download(root=CHECKPOINT_DIR)\n",
    "modelA = BaseModel.load_from_checkpoint(Path(artifact_dir)/\"model.ckpt\")\n",
    "\n",
    "artifact = run.use_artifact(checkpointB, type='model')\n",
    "artifact_dir = artifact.download(root=CHECKPOINT_DIR)\n",
    "modelB = BaseModel.load_from_checkpoint(Path(artifact_dir)/\"model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Computing LMC barrier before alignment-------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.00 (model 2), Train average loss: 0.17894 Train barrier:  0\n",
      "Alpha: 1.00 (model 1), Train average loss: 0.13663 Train barrier:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.05, Train average loss: 0.27370 Train barrier 0.0968766831672192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.10, Train average loss: 0.63219 Train barrier 0.4574857050093014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.15, Train average loss: 1.22188 Train barrier 1.049286051843034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.20, Train average loss: 1.76548 Train barrier 1.5950022199853262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.25, Train average loss: 2.08447 Train barrier 1.9161087374733554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.30, Train average loss: 2.22626 Train barrier 2.060015648129516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.35, Train average loss: 2.27955 Train barrier 2.115424966368278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.40, Train average loss: 2.29688 Train barrier 2.134869789477984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.45, Train average loss: 2.30158 Train barrier 2.141682490251197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.50, Train average loss: 2.30246 Train barrier 2.1446790701137646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.55, Train average loss: 2.30183 Train barrier 2.146156465990941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.60, Train average loss: 2.29791 Train barrier 2.144355836346944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.65, Train average loss: 2.28294 Train barrier 2.13149858120958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.70, Train average loss: 2.23558 Train barrier 2.0862537292212906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.75, Train average loss: 2.10581 Train barrier 1.9585964801450568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.80, Train average loss: 1.80240 Train barrier 1.6573089058362114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.85, Train average loss: 1.25767 Train barrier 1.114689681132502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.90, Train average loss: 0.63731 Train barrier 0.4964477657829391\n",
      "Alpha: 0.95, Train average loss: 0.25172 Train barrier 0.11297376769026118\n",
      "Loss model 1: 0.13663, Loss model 2: 0.17894, Alpha argmax: 0.55000\n",
      "Barrier: 2.14616\n"
     ]
    }
   ],
   "source": [
    "# LMC barrier\n",
    "print(\"------- Computing LMC barrier before alignment-------\")\n",
    "\n",
    "lmc_experiment.run_lmc(\n",
    "    datamodule_type=datamodule_type,\n",
    "    modelA=modelA,\n",
    "    modelB=modelB,\n",
    "    granularity=21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Computing baselines -------\n",
      "------- Prediction based ensembling -------\n",
      "------- Naive ensembling of weights -------\n",
      "------- Evaluating baselines -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating base models -------\n",
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:01<00:00,  5.95it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy          0.8513000011444092\n",
      "        val_loss            0.5394402146339417\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:01<00:00,  7.14it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy           0.852400004863739\n",
      "        val_loss            0.5292244553565979\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "------- Evaluating prediction ensembling -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.4542, Accuracy: 87.15%\n",
      "------- Evaluating vanilla averaging -------\n",
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:01<00:00,  5.05it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy          0.32760000228881836\n",
      "        val_loss             2.302485227584839\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁</td></tr><tr><td>val_accuracy</td><td>██▁</td></tr><tr><td>val_loss</td><td>▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>0</td></tr><tr><td>val_accuracy</td><td>0.3276</td></tr><tr><td>val_loss</td><td>2.30249</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-feather-207</strong> at: <a href='https://wandb.ai/model-fusion/model-fusion/runs/rnkmrxv1' target=\"_blank\">https://wandb.ai/model-fusion/model-fusion/runs/rnkmrxv1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240112_121623-rnkmrxv1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Baselines (prediction ensembling, vanilla averaging)\n",
    "print(\"------- Computing baselines -------\")\n",
    "\n",
    "wandb_tag = f'baselines-{runA}-{runB}'\n",
    "\n",
    "vanilla_averaging_model = baselines_experiment.run_baselines(\n",
    "    datamodule_type=datamodule_type,\n",
    "    datamodule_hparams=datamodule_hparams,\n",
    "    model_type=model_type, \n",
    "    model_hparams=model_hparams,\n",
    "    modelA=modelA,\n",
    "    modelB=modelB,\n",
    "    wandb_tag=wandb_tag,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Computing model fusion -------\n",
      "------- Setting up parameters -------\n",
      "{'seed': 42, 'data_dir': 'data', 'batch_size': 32, 'data_augmentation': False}\n",
      "The parameters are: \n",
      " {'eval_aligned': True, 'num_models': 2, 'width_ratio': 1, 'handle_skips': False, 'exact': True, 'activation_seed': 21, 'activation_histograms': True, 'ground_metric': 'euclidean', 'ground_metric_normalize': 'none', 'same_model': False, 'geom_ensemble_type': 'acts', 'act_num_samples': 75, 'skip_last_layer': False, 'skip_last_layer_type': 'average', 'softmax_temperature': 1, 'past_correction': True, 'correction': True, 'normalize_acts': False, 'normalize_wts': False, 'activation_normalize': False, 'center_acts': False, 'prelu_acts': False, 'pool_acts': False, 'pool_relu': False, 'importance': None, 'proper_marginals': False, 'not_squared': True, 'dist_normalize': False, 'clip_gm': False, 'clip_min': 0, 'clip_max': 5, 'tmap_stats': False, 'ensemble_step': 0.5, 'ground_metric_eff': False, 'reg': 0.01}\n",
      "------- OT model fusion -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Computing activations\n",
      "this was continued,  \n",
      "this was continued,  loss_module\n",
      "this was continued,  accuracy\n",
      "this was continued,  model\n",
      "this was continued,  model.features\n",
      "set forward hook for layer named:  model.features.0\n",
      "this was continued,  model.features.1\n",
      "this was continued,  model.features.2\n",
      "set forward hook for layer named:  model.features.3\n",
      "this was continued,  model.features.4\n",
      "this was continued,  model.features.5\n",
      "set forward hook for layer named:  model.features.6\n",
      "this was continued,  model.features.7\n",
      "set forward hook for layer named:  model.features.8\n",
      "this was continued,  model.features.9\n",
      "this was continued,  model.features.10\n",
      "set forward hook for layer named:  model.features.11\n",
      "this was continued,  model.features.12\n",
      "set forward hook for layer named:  model.features.13\n",
      "this was continued,  model.features.14\n",
      "this was continued,  model.features.15\n",
      "set forward hook for layer named:  model.features.16\n",
      "this was continued,  model.features.17\n",
      "set forward hook for layer named:  model.features.18\n",
      "this was continued,  model.features.19\n",
      "this was continued,  model.features.20\n",
      "this was continued,  model.avgpool\n",
      "this was continued,  model.classifier\n",
      "set forward hook for layer named:  model.classifier.0\n",
      "this was continued,  model.classifier.1\n",
      "this was continued,  model.classifier.2\n",
      "set forward hook for layer named:  model.classifier.3\n",
      "this was continued,  model.classifier.4\n",
      "this was continued,  model.classifier.5\n",
      "set forward hook for layer named:  model.classifier.6\n",
      "this was continued,  \n",
      "this was continued,  loss_module\n",
      "this was continued,  accuracy\n",
      "this was continued,  model\n",
      "this was continued,  model.features\n",
      "set forward hook for layer named:  model.features.0\n",
      "this was continued,  model.features.1\n",
      "this was continued,  model.features.2\n",
      "set forward hook for layer named:  model.features.3\n",
      "this was continued,  model.features.4\n",
      "this was continued,  model.features.5\n",
      "set forward hook for layer named:  model.features.6\n",
      "this was continued,  model.features.7\n",
      "set forward hook for layer named:  model.features.8\n",
      "this was continued,  model.features.9\n",
      "this was continued,  model.features.10\n",
      "set forward hook for layer named:  model.features.11\n",
      "this was continued,  model.features.12\n",
      "set forward hook for layer named:  model.features.13\n",
      "this was continued,  model.features.14\n",
      "this was continued,  model.features.15\n",
      "set forward hook for layer named:  model.features.16\n",
      "this was continued,  model.features.17\n",
      "set forward hook for layer named:  model.features.18\n",
      "this was continued,  model.features.19\n",
      "this was continued,  model.features.20\n",
      "this was continued,  model.avgpool\n",
      "this was continued,  model.classifier\n",
      "set forward hook for layer named:  model.classifier.0\n",
      "this was continued,  model.classifier.1\n",
      "this was continued,  model.classifier.2\n",
      "set forward hook for layer named:  model.classifier.3\n",
      "this was continued,  model.classifier.4\n",
      "this was continued,  model.classifier.5\n",
      "set forward hook for layer named:  model.classifier.6\n",
      "Activations computed across 75 samples out of 45000\n",
      "***********\n",
      "min of act: 0.0, max: 16.08638572692871, mean: 0.3055078685283661\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 16.08638572692871, mean: 0.3055078685283661\n",
      "***********\n",
      "min of act: 0.0, max: 43.91558074951172, mean: 0.2850857675075531\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 43.91558074951172, mean: 0.2850857675075531\n",
      "***********\n",
      "min of act: 0.0, max: 77.24015808105469, mean: 0.5082271695137024\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 77.24015808105469, mean: 0.5082271695137024\n",
      "***********\n",
      "min of act: 0.0, max: 66.45433807373047, mean: 0.19704724848270416\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 66.45433807373047, mean: 0.19704724848270416\n",
      "***********\n",
      "min of act: 0.0, max: 95.75228881835938, mean: 0.22729575634002686\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 95.75228881835938, mean: 0.22729575634002686\n",
      "***********\n",
      "min of act: 0.0, max: 49.347023010253906, mean: 0.05717979371547699\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 49.347023010253906, mean: 0.05717979371547699\n",
      "***********\n",
      "min of act: 0.0, max: 47.55022430419922, mean: 0.04641135036945343\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 47.55022430419922, mean: 0.04641135036945343\n",
      "***********\n",
      "min of act: 0.0, max: 15.966678619384766, mean: 0.02156895399093628\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 15.966678619384766, mean: 0.02156895399093628\n",
      "***********\n",
      "min of act: 0.0, max: 29.899311065673828, mean: 0.11355029791593552\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 29.899311065673828, mean: 0.11355029791593552\n",
      "***********\n",
      "min of act: 0.0, max: 17.49948501586914, mean: 0.11562320590019226\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 17.49948501586914, mean: 0.11562320590019226\n",
      "***********\n",
      "min of act: -49.627357482910156, max: 80.71514892578125, mean: 0.014359421096742153\n",
      "***********\n",
      "min of act: 0.0, max: 16.23641014099121, mean: 0.2990298867225647\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 16.23641014099121, mean: 0.2990298867225647\n",
      "***********\n",
      "min of act: 0.0, max: 35.878746032714844, mean: 0.27007660269737244\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 35.878746032714844, mean: 0.27007660269737244\n",
      "***********\n",
      "min of act: 0.0, max: 69.46148681640625, mean: 0.47330302000045776\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 69.46148681640625, mean: 0.47330302000045776\n",
      "***********\n",
      "min of act: 0.0, max: 77.08943939208984, mean: 0.1948540210723877\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 77.08943939208984, mean: 0.1948540210723877\n",
      "***********\n",
      "min of act: 0.0, max: 76.58497619628906, mean: 0.1891304850578308\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 76.58497619628906, mean: 0.1891304850578308\n",
      "***********\n",
      "min of act: 0.0, max: 50.86299133300781, mean: 0.05521942675113678\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 50.86299133300781, mean: 0.05521942675113678\n",
      "***********\n",
      "min of act: 0.0, max: 47.60951614379883, mean: 0.042387474328279495\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 47.60951614379883, mean: 0.042387474328279495\n",
      "***********\n",
      "min of act: 0.0, max: 23.057926177978516, mean: 0.02150743082165718\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 23.057926177978516, mean: 0.02150743082165718\n",
      "***********\n",
      "min of act: 0.0, max: 30.240171432495117, mean: 0.11531968414783478\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 30.240171432495117, mean: 0.11531968414783478\n",
      "***********\n",
      "min of act: 0.0, max: 12.02674388885498, mean: 0.11262670904397964\n",
      "applying relu ---------------\n",
      "after RELU: min of act: 0.0, max: 12.02674388885498, mean: 0.11262670904397964\n",
      "***********\n",
      "min of act: -30.701396942138672, max: 62.40270233154297, mean: -0.006186624988913536\n",
      "activations for idx 1 at layer model.classifier.6 have the following shape  torch.Size([75, 1, 10])\n",
      "-----------\n",
      "INIT\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 0 ------------- \n",
      " \n",
      "Previous layer shape is  None\n",
      "In layer model.features.0.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 11.376529693603516, Mean : 3.710277795791626, Min : 0.709244966506958, Std: 2.1454412937164307\n",
      "At layer idx 0 and shape torch.Size([64, 3, 3, 3]), the OT cost is  96.93518710136414\n",
      "Tmap stats (before correction) \n",
      ": For layer model.features.0.weight, frobenius norm from the joe's transport map is 0.12401959300041199\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0156]])\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624897554516792, std 0.12403393536806107 \n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 1 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "In layer model.features.3.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 18.65406036376953, Mean : 4.105411529541016, Min : 0.07487454265356064, Std: 3.803138256072998\n",
      "At layer idx 1 and shape torch.Size([128, 64, 3, 3]), the OT cost is  129.03221048865817\n",
      "Tmap stats (before correction) \n",
      ": For layer model.features.3.weight, frobenius norm from the joe's transport map is 0.08804240077733994\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078]])\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812399882823229, std 0.08804396539926529 \n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 2 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "In layer model.features.6.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 25.298564910888672, Mean : 4.64259672164917, Min : 0.008764472790062428, Std: 5.060244560241699\n",
      "At layer idx 2 and shape torch.Size([256, 128, 3, 3]), the OT cost is  134.28311046783347\n",
      "Tmap stats (before correction) \n",
      ": For layer model.features.6.weight, frobenius norm from the joe's transport map is 0.06237781047821045\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237668916583061 \n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 3 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "In layer model.features.8.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 19.76620101928711, Mean : 2.285749912261963, Min : 7.275922143890057e-06, Std: 3.750685691833496\n",
      "At layer idx 3 and shape torch.Size([256, 256, 3, 3]), the OT cost is  97.0335513035534\n",
      "Tmap stats (before correction) \n",
      ": For layer model.features.8.weight, frobenius norm from the joe's transport map is 0.06237781047821045\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.0039061501156538725, std 0.06237668916583061 \n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 4 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "In layer model.features.11.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 12.60509967803955, Mean : 1.3090267181396484, Min : 7.3527212407498155e-06, Std: 2.319502830505371\n",
      "At layer idx 4 and shape torch.Size([512, 256, 3, 3]), the OT cost is  36.829182493675034\n",
      "Tmap stats (before correction) \n",
      ": For layer model.features.11.weight, frobenius norm from the joe's transport map is 0.04415099322795868\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530252320691943, std 0.044148821383714676 \n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 5 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "In layer model.features.13.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 5.670825481414795, Mean : 0.38319292664527893, Min : 0.0, Std: 0.9550052285194397\n",
      "At layer idx 5 and shape torch.Size([512, 512, 3, 3]), the OT cost is  14.474698711994279\n",
      "Tmap stats (before correction) \n",
      ": For layer model.features.13.weight, frobenius norm from the joe's transport map is 0.04415099322795868\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530252320691943, std 0.044148821383714676 \n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 6 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "In layer model.features.16.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 2.471560478210449, Mean : 0.16624757647514343, Min : 0.0, Std: 0.42265623807907104\n",
      "At layer idx 6 and shape torch.Size([512, 512, 3, 3]), the OT cost is  3.5402134875221236\n",
      "Tmap stats (before correction) \n",
      ": For layer model.features.16.weight, frobenius norm from the joe's transport map is 0.04415099322795868\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 7 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "In layer model.features.18.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 1.3226890563964844, Mean : 0.08147972822189331, Min : 0.0, Std: 0.22011539340019226\n",
      "At layer idx 7 and shape torch.Size([512, 512, 3, 3]), the OT cost is  1.79149972048981\n",
      "Tmap stats (before correction) \n",
      ": For layer model.features.18.weight, frobenius norm from the joe's transport map is 0.04415099322795868\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530252320691943, std 0.044148821383714676 \n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 8 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "In layer model.classifier.0.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 44.74810028076172, Mean : 4.639777183532715, Min : 1.0699530839920044, Std: 4.830976486206055\n",
      "At layer idx 8 and shape torch.Size([4096, 25088]), the OT cost is  1.354716101776262\n",
      "Tmap stats (before correction) \n",
      ": For layer model.classifier.0.weight, frobenius norm from the joe's transport map is 0.015621190890669823\n",
      "shape of T_var is  torch.Size([4096, 4096])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "T_var stats: max 0.9995905756950378, min 0.0, mean 0.0002440406387904659, std 0.015616696327924728 \n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Shape of aligned wt is  torch.Size([4096, 25088])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 9 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([4096, 25088])\n",
      "In layer model.classifier.3.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 18.99512481689453, Mean : 3.9459266662597656, Min : 1.3188517093658447, Std: 2.1609444618225098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\ot\\lp\\__init__.py:354: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At layer idx 9 and shape torch.Size([4096, 4096]), the OT cost is  1.604075084253651\n",
      "Tmap stats (before correction) \n",
      ": For layer model.classifier.3.weight, frobenius norm from the joe's transport map is 0.015621190890669823\n",
      "shape of T_var is  torch.Size([4096, 4096])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.9996,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "T_var stats: max 0.9995905756950378, min 0.0, mean 0.00024404065334238112, std 0.015616696327924728 \n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.9996,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "Shape of aligned wt is  torch.Size([4096, 4096])\n",
      "NUM LAYERS:  11\n",
      "\n",
      "--------------- At layer index 10 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([4096, 4096])\n",
      "In layer model.classifier.6.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 141.73141479492188, Mean : 99.93232727050781, Min : 40.06764602661133, Std: 30.540719985961914\n",
      "At layer idx 10 and shape torch.Size([10, 4096]), the OT cost is  40.06764602661133\n",
      "Tmap stats (before correction) \n",
      ": For layer model.classifier.6.weight, frobenius norm from the joe's transport map is 0.30000001192092896\n",
      "shape of T_var is  torch.Size([10, 10])\n",
      "T_var before correction  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]])\n",
      "T_var after correction  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000]])\n",
      "T_var stats: max 0.9999990463256836, min 0.0, mean 0.09999990463256836, std 0.3015110492706299 \n",
      "the transport map is  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000]])\n",
      "Shape of aligned wt is  torch.Size([10, 4096])\n",
      "EXIT\n",
      "len of model_state_dict is  11\n",
      "len of new_params is  11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240112_121858-4rlk065s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/model-fusion/Model%20Fusion/runs/4rlk065s' target=\"_blank\">vgg11_cifar10_batch_size_32_aligned_modelA</a></strong> to <a href='https://wandb.ai/model-fusion/Model%20Fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/model-fusion/Model%20Fusion' target=\"_blank\">https://wandb.ai/model-fusion/Model%20Fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/model-fusion/Model%20Fusion/runs/4rlk065s' target=\"_blank\">https://wandb.ai/model-fusion/Model%20Fusion/runs/4rlk065s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 313/313 [00:03<00:00, 97.21it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy          0.8511000275611877\n",
      "        val_loss            0.5388107895851135\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>0</td></tr><tr><td>val_accuracy</td><td>0.8511</td></tr><tr><td>val_loss</td><td>0.53881</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vgg11_cifar10_batch_size_32_aligned_modelA</strong> at: <a href='https://wandb.ai/model-fusion/Model%20Fusion/runs/4rlk065s' target=\"_blank\">https://wandb.ai/model-fusion/Model%20Fusion/runs/4rlk065s</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240112_121858-4rlk065s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# model parameters:  11\n",
      "# new parameters:  11\n",
      "fusing:  model.features.0.weight\n",
      "fusing:  model.features.3.weight\n",
      "fusing:  model.features.6.weight\n",
      "fusing:  model.features.8.weight\n",
      "fusing:  model.features.11.weight\n",
      "fusing:  model.features.13.weight\n",
      "fusing:  model.features.16.weight\n",
      "fusing:  model.features.18.weight\n",
      "fusing:  model.classifier.0.weight\n",
      "fusing:  model.classifier.3.weight\n",
      "fusing:  model.classifier.6.weight\n",
      "------- Evaluating ot fusion model -------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240112_121910-8e7oxb3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/model-fusion/Model%20Fusion/runs/8e7oxb3z' target=\"_blank\">vgg11_cifar10_batch_size_32_ot_model_fusion-wil30lcb-6v6im8ni</a></strong> to <a href='https://wandb.ai/model-fusion/Model%20Fusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/model-fusion/Model%20Fusion' target=\"_blank\">https://wandb.ai/model-fusion/Model%20Fusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/model-fusion/Model%20Fusion/runs/8e7oxb3z' target=\"_blank\">https://wandb.ai/model-fusion/Model%20Fusion/runs/8e7oxb3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 313/313 [00:02<00:00, 104.99it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy          0.6837000250816345\n",
      "        val_loss            1.8715574741363525\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>trainer/global_step</td><td>0</td></tr><tr><td>val_accuracy</td><td>0.6837</td></tr><tr><td>val_loss</td><td>1.87156</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vgg11_cifar10_batch_size_32_ot_model_fusion-wil30lcb-6v6im8ni</strong> at: <a href='https://wandb.ai/model-fusion/Model%20Fusion/runs/8e7oxb3z' target=\"_blank\">https://wandb.ai/model-fusion/Model%20Fusion/runs/8e7oxb3z</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240112_121910-8e7oxb3z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OT model fusion + eval aligned model \n",
    "print(\"------- Computing model fusion -------\")\n",
    "\n",
    "wandb_tag = f\"ot_model_fusion-{runA}-{runB}\"\n",
    "\n",
    "ot_fused_model, modelA_aligned = otfusion_experiment.run_otfusion(\n",
    "    batch_size=batch_size,\n",
    "    datamodule_type=datamodule_type,\n",
    "    datamodule_hparams=datamodule_hparams,\n",
    "    model_type=model_type, \n",
    "    model_hparams=model_hparams,\n",
    "    modelA=modelA,\n",
    "    modelB=modelB,\n",
    "    wandb_tag=wandb_tag,\n",
    "    is_vgg=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'C:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\VGG_cifar10_models'\n",
    "modelOTF_name = 'modelOTF_32AB_VGG.t7'\n",
    "torch.save(ot_fused_model.state_dict(), save_path + '\\\\' + modelOTF_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Computing LMC barrier after alignment -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.00 (model 2), Train average loss: 0.17894 Train barrier:  0\n",
      "Alpha: 1.00 (model 1), Train average loss: 0.13755 Train barrier:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.05, Train average loss: 0.22857 Train barrier 0.0517010951034228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.10, Train average loss: 0.39016 Train barrier 0.21536367941008672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.15, Train average loss: 0.65574 Train barrier 0.48301175575812655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.20, Train average loss: 0.96817 Train barrier 0.7975120430146324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.25, Train average loss: 1.26121 Train barrier 1.0926186002135276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.30, Train average loss: 1.49606 Train barrier 1.3295362230968477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.35, Train average loss: 1.66439 Train barrier 1.4999438170138997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.40, Train average loss: 1.77342 Train barrier 1.6110368290175332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.45, Train average loss: 1.83276 Train barrier 1.6724428954238362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.50, Train average loss: 1.85353 Train barrier 1.6952870882034303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.55, Train average loss: 1.83896 Train barrier 1.6827811161033313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.60, Train average loss: 1.78615 Train barrier 1.6320483386129805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.65, Train average loss: 1.68635 Train barrier 1.5343184997322825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.70, Train average loss: 1.52647 Train barrier 1.376500099984275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.75, Train average loss: 1.29471 Train barrier 1.1468123110201625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.80, Train average loss: 0.99540 Train barrier 0.8495677761242125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.85, Train average loss: 0.66775 Train barrier 0.5239919560164875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.90, Train average loss: 0.38355 Train barrier 0.24186264839066401\n",
      "Alpha: 0.95, Train average loss: 0.20421 Train barrier 0.06458589858293531\n",
      "Loss model 1: 0.13755, Loss model 2: 0.17894, Alpha argmax: 0.50000\n",
      "Barrier: 1.69529\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla loss pre fine-tuning: 2.3024632548014323\n",
      "Fused loss pre fine-tuning: 1.8532997225019667\n"
     ]
    }
   ],
   "source": [
    "# LMC barrier\n",
    "print(\"------- Computing LMC barrier after alignment -------\")\n",
    "\n",
    "lmc_experiment.run_lmc(\n",
    "    datamodule_type=datamodule_type,\n",
    "    modelA=modelA_aligned,\n",
    "    modelB=modelB,\n",
    "    granularity=21\n",
    ")\n",
    "\n",
    "# Losses for ot fusion model and vanilla averaging model\n",
    "datamodule_hparams_lmc = {'batch_size': 1024, 'data_dir': BASE_DATA_DIR}\n",
    "datamodule_lmc = datamodule_type.get_data_module(**datamodule_hparams)\n",
    "datamodule_lmc.prepare_data()\n",
    "datamodule_lmc.setup('fit')\n",
    "\n",
    "vanilla_loss = lmc_utils.compute_loss(vanilla_averaging_model, datamodule_lmc)\n",
    "fused_loss = lmc_utils.compute_loss(ot_fused_model, datamodule_lmc)\n",
    "\n",
    "print(f\"Vanilla loss pre fine-tuning: {vanilla_loss}\")\n",
    "print(f\"Fused loss pre fine-tuning: {fused_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Computing sharpness -------\n",
      "------- Model B -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ..\\torch\\csrc\\autograd\\engine.cpp:1176.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top Hessian eigenvalue of this model is 14.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Trace:  659.6372572954963\n",
      "------- Model A aligned to B -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The top Hessian eigenvalue of this model is 13.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Trace:  616.1067186438519\n",
      "------- Vanilla avg model -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The top Hessian eigenvalue of this model is -0.0083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Trace:  -0.009973247266644912\n",
      "------- OT fusion model -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "The top Hessian eigenvalue of this model is 3.4513\n",
      "\n",
      "***Trace:  22.460331320762634\n"
     ]
    }
   ],
   "source": [
    "# Pyhessian (compute sharpness and eigenspectrum of base models, vanilla avg, ot fusion and finetuned solutions)\n",
    "print(\"------- Computing sharpness -------\")\n",
    "\n",
    "print(\"------- Model A -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=modelA, compute_density=False, figure_name='modelA.pdf') \n",
    "\n",
    "print(\"------- Model B -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=modelB, compute_density=False, figure_name='modelB.pdf')\n",
    "\n",
    "print(\"------- Model A aligned to B -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type,model=modelA_aligned,  compute_density=False, figure_name='modelA_aligned.pdf')\n",
    "\n",
    "print(\"------- Vanilla avg model -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type,model=vanilla_averaging_model,  compute_density=False, figure_name='vanilla_avg.pdf')\n",
    "\n",
    "print(\"------- OT fusion model -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=ot_fused_model,  compute_density=False, figure_name='otmodel_same_init32_VGG.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'C:\\Users\\filos\\OneDrive\\Desktop\\ETH\\model-fusion\\Vgg_cifar10_models'\n",
    "\n",
    "modelB_name = 'model32B_VGG.t7'\n",
    "torch.save(modelB.state_dict(), save_path + '\\\\' + modelB_name)\n",
    "\n",
    "modelA_aligned_name = 'model32A_aligned_to32B_VGG.t7'\n",
    "torch.save(modelA_aligned.state_dict(), save_path + '\\\\' + modelA_aligned_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
