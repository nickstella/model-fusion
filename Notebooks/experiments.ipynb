{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from Experiments import baselines_experiment\n",
    "from Experiments import lmc_experiment\n",
    "from Experiments import otfusion_experiment\n",
    "from Experiments import pyhessian_experiment\n",
    "from model_fusion import lmc_utils\n",
    "from model_fusion.config import BASE_DATA_DIR, CHECKPOINT_DIR\n",
    "from model_fusion.datasets import DataModuleType\n",
    "from model_fusion.models import ModelType\n",
    "from model_fusion.models.lightning import BaseModel\n",
    "from model_fusion.train import setup_training, setup_testing, get_wandb_logger\n",
    "\n",
    "# set seed for numpy based calculations\n",
    "NUMPY_SEED = 100\n",
    "np.random.seed(NUMPY_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already provide the finetuned models through WandB, however, the `enable_finetuning` flag can be set to True to perform finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_finetuning = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-18, Cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Batch Sizes 32 and 32\n",
    "resnet_cifar10_same_init_32_32_config = {\n",
    "    \"model_a_run\": \"bbecqkxs\",\n",
    "    \"model_b_run\": \"uw0w0e3e\",\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 128 and 128\n",
    "resnet_cifar10_same_init_128_128_config = {\n",
    "    \"model_a_run\": \"3bsofnmw\",\n",
    "    \"model_b_run\": \"zp0c8n4p\",\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 512 and 512\n",
    "resnet_cifar10_same_init_512_512_config = {\n",
    "    \"model_a_run\": \"kvuejplb\",\n",
    "    \"model_b_run\": \"kwdhgbfv\",\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 512 and 32\n",
    "resnet_cifar10_same_init_512_32_config = {\n",
    "    \"model_a_run\": \"kvuejplb\",\n",
    "    \"model_b_run\": \"bbecqkxs\",\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 32 and 512\n",
    "resnet_cifar10_same_init_32_512_config = {\n",
    "    \"model_a_run\": \"bbecqkxs\",\n",
    "    \"model_b_run\": \"kvuejplb\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Batch Sizes 32 and 32\n",
    "resnet_cifar10_diff_init_32_32_config = {\n",
    "    \"model_a_run\": \"bbecqkxs\",\n",
    "    \"model_b_run\": \"k9q16yq1\",\n",
    "    \"vanilla_averaging_model_finetuning_run\": \"sm4quce8\",\n",
    "    \"ot_fused_model_finetuning_run\": \"0a6mj32n\",\n",
    "    \"finetuning_batch_size\": 32,\n",
    "    \"vanilla_finetuning_lr\": 0.01,\n",
    "    \"vanilla_finetuning_momentum\": 0.95,\n",
    "    \"otf_finetuning_lr\": 0.01,\n",
    "    \"otf_finetuning_momentum\": 0.95,\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 128 and 128\n",
    "resnet_cifar10_diff_init_128_128_config = {\n",
    "    \"model_a_run\": \"3bsofnmw\",\n",
    "    \"model_b_run\": \"q2135wcz\",\n",
    "    \"vanilla_averaging_model_finetuning_run\": \"30nfxxwe\",\n",
    "    \"ot_fused_model_finetuning_run\": \"8z426q3y\",\n",
    "    \"finetuning_batch_size\": 128,\n",
    "    \"vanilla_finetuning_lr\": 0.05,\n",
    "    \"vanilla_finetuning_momentum\": 0.9,\n",
    "    \"otf_finetuning_lr\": 0.05,\n",
    "    \"otf_finetuning_momentum\": 0.9,\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 512 and 512\n",
    "resnet_cifar10_diff_init_512_512_config = {\n",
    "    \"model_a_run\": \"kvuejplb\",\n",
    "    \"model_b_run\": \"yqpgz3ya\",\n",
    "    \"vanilla_averaging_model_finetuning_run\": \"39ttcid9\",\n",
    "    \"ot_fused_model_finetuning_run\": \"xbppg39l\",\n",
    "    \"finetuning_batch_size\": 512,\n",
    "    \"vanilla_finetuning_lr\": 0.1,\n",
    "    \"vanilla_finetuning_momentum\": 0.9,\n",
    "    \"otf_finetuning_lr\": 0.1,\n",
    "    \"otf_finetuning_momentum\": 0.9,\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 512 and 32\n",
    "resnet_cifar10_diff_init_512_32_config = {\n",
    "    \"model_a_run\": \"kvuejplb\",\n",
    "    \"model_b_run\": \"k9q16yq1\",\n",
    "    # FT batch size 32\n",
    "    \"ot_fused_model_finetuning_run\": \"wc7z0q4u\",\n",
    "    \"finetuning_batch_size\": 32,\n",
    "    \"otf_finetuning_lr\": 0.01,\n",
    "    \"otf_finetuning_momentum\": 0.9,\n",
    "    # FT batch size 256 - uncomment this part and comment the above part to run with batch size 256\n",
    "    # \"ot_fused_model_finetuning_run\": \"is4t96nh\",\n",
    "    # \"finetuning_batch_size\": 256,\n",
    "    # \"otf_finetuning_lr\": 0.1,\n",
    "    # \"otf_finetuning_momentum\": 0.9,\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 32 and 512\n",
    "resnet_cifar10_diff_init_32_512_config = {\n",
    "    \"model_a_run\": \"k9q16yq1\",\n",
    "    \"model_b_run\": \"kvuejplb\",\n",
    "    # FT batch size 512\n",
    "    \"ot_fused_model_finetuning_run\": \"yzai8540\",\n",
    "    \"finetuning_batch_size\": 512,\n",
    "    \"otf_finetuning_lr\": 0.1,\n",
    "    \"otf_finetuning_momentum\": 0.9,\n",
    "    # FT batch size 256 - uncomment this part and comment the above part to run with batch size 256\n",
    "    # \"ot_fused_model_finetuning_run\": \"3yaujh7p\",\n",
    "    # \"finetuning_batch_size\": 256,\n",
    "    # \"otf_finetuning_lr\": 0.1,\n",
    "    # \"otf_finetuning_momentum\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-18, MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Batch Sizes 32 and 32\n",
    "resnet_mnist_same_init_32_32_config = {\n",
    "    \"model_a_run\": \"xrjk55ng\",\n",
    "    \"model_b_run\": \"bmt8o992\",\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 512 and 512\n",
    "resnet_mnist_same_init_512_512_config = {\n",
    "    \"model_a_run\": \"djhsbo0l\",\n",
    "    \"model_b_run\": \"d7g313hy\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Batch Sizes 32 and 32\n",
    "resnet_mnist_diff_init_32_32_config = {\n",
    "    \"model_a_run\": \"xrjk55ng\",\n",
    "    \"model_b_run\": \"u2ckjxvf\",\n",
    "    \"ot_fused_model_finetuning_run\": \"k6rv9lh7\",\n",
    "    \"finetuning_batch_size\": 32,\n",
    "    \"otf_finetuning_lr\": 0.005,\n",
    "    \"otf_finetuning_momentum\": 0.98,\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 512 and 512\n",
    "resnet_mnist_diff_init_512_512_config = {\n",
    "    \"model_a_run\": \"djhsbo0l\",\n",
    "    \"model_b_run\": \"xzolnqeo\",\n",
    "    \"ot_fused_model_finetuning_run\": \"insax4t9\",\n",
    "    \"finetuning_batch_size\": 512,\n",
    "    \"otf_finetuning_lr\": 0.1,\n",
    "    \"otf_finetuning_momentum\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-11, CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Batch Sizes 32 and 32\n",
    "vgg_cifar10_same_init_32_32_config = {\n",
    "    \"model_a_run\": \"wil30lcb\",\n",
    "    \"model_b_run\": \"6v6im8ni\",\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 512 and 512\n",
    "vgg_cifar10_same_init_512_512_config = {\n",
    "    \"model_a_run\": \"33kyx0p1\",\n",
    "    \"model_b_run\": \"3ezo4au5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Batch Sizes 32 and 32\n",
    "vgg_cifar10_diff_init_32_32_config = {\n",
    "    \"model_a_run\": \"wil30lcb\",\n",
    "    \"model_b_run\": \"0z3nowvr\",\n",
    "    \"ot_fused_model_finetuning_run\": \"xhyln9am\",\n",
    "    \"finetuning_batch_size\": 32,\n",
    "    \"otf_finetuning_lr\": 0.005,\n",
    "    \"otf_finetuning_momentum\": 0.95,\n",
    "}\n",
    "\n",
    "# Parent Batch Sizes 512 and 512\n",
    "vgg_cifar10_diff_init_512_512_config = {\n",
    "    \"model_a_run\": \"33kyx0p1\",\n",
    "    \"model_b_run\": \"5tbujwji\",\n",
    "    \"ot_fused_model_finetuning_run\": \"oq06dgmw\",\n",
    "    \"finetuning_batch_size\": 512,\n",
    "    \"otf_finetuning_lr\": 0.1,\n",
    "    \"otf_finetuning_momentum\": 0.95,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the desired experiment config here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = resnet_cifar10_diff_init_32_32_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent model runs on WandB\n",
    "runA = experiment_config[\"model_a_run\"]\n",
    "runB = experiment_config[\"model_b_run\"]\n",
    "\n",
    "# Finetuning runs on WandB\n",
    "vanilla_averaging_model_finetuning_run = experiment_config.get(\"vanilla_averaging_model_finetuning_run\", None)\n",
    "ot_fused_model_finetuning_run = experiment_config.get(\"ot_fused_model_finetuning_run\", None)\n",
    "\n",
    "# Finetuning parameters\n",
    "finetuning_batch_size = experiment_config.get(\"finetuning_batch_size\", None)\n",
    "vanilla_finetuning_lr = experiment_config.get(\"vanilla_finetuning_lr\", None)\n",
    "vanilla_finetuning_momentum = experiment_config.get(\"vanilla_finetuning_momentum\", None)\n",
    "otf_finetuning_lr = experiment_config.get(\"otf_finetuning_lr\", None)\n",
    "otf_finetuning_momentum = experiment_config.get(\"otf_finetuning_momentum\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(f'model-fusion/Model Fusion/{runA}')\n",
    "\n",
    "print(run.config)\n",
    "\n",
    "batch_size = run.config['datamodule_hparams'].get('batch_size')\n",
    "\n",
    "datamodule_type_str = run.config['datamodule_type'].split('.')[1].lower()\n",
    "datamodule_type = DataModuleType(datamodule_type_str)\n",
    "datamodule_hparams = run.config['datamodule_hparams']\n",
    "datamodule_hparams['data_augmentation'] = False\n",
    "\n",
    "model_type_str = run.config['model_type'].split('.')[1].lower()\n",
    "model_type = ModelType(model_type_str)\n",
    "\n",
    "model_hparams = run.config['model_hparams']\n",
    "\n",
    "print(datamodule_hparams)\n",
    "print(model_hparams)\n",
    "\n",
    "checkpointA = f'model-fusion/Model Fusion/model-{runA}:best_k'\n",
    "checkpointB = f'model-fusion/Model Fusion/model-{runB}:best_k'\n",
    "\n",
    "run = wandb.init()\n",
    "\n",
    "artifact = run.use_artifact(checkpointA, type='model')\n",
    "artifact_dir = artifact.download(root=CHECKPOINT_DIR)\n",
    "modelA = BaseModel.load_from_checkpoint(Path(artifact_dir)/\"model.ckpt\")\n",
    "\n",
    "artifact = run.use_artifact(checkpointB, type='model')\n",
    "artifact_dir = artifact.download(root=CHECKPOINT_DIR)\n",
    "modelB = BaseModel.load_from_checkpoint(Path(artifact_dir)/\"model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMC Barrier before alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmc_experiment.run_lmc(\n",
    "    datamodule_type=datamodule_type,\n",
    "    modelA=modelA,\n",
    "    modelB=modelB,\n",
    "    granularity=21\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test Accuracies\n",
    "\n",
    "Here we compute the test accuracies for the parent models. Additionally, we create the vanilla averaging and OT fused models, and evaluate their test accuracies as well. We ignore the test loss as we only consider the training loss, which we compute later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_tag = f'baselines-{runA}-{runB}'\n",
    "\n",
    "vanilla_averaging_model = baselines_experiment.run_baselines(\n",
    "    datamodule_type=datamodule_type,\n",
    "    datamodule_hparams=datamodule_hparams,\n",
    "    model_type=model_type,\n",
    "    model_hparams=model_hparams,\n",
    "    modelA=modelA,\n",
    "    modelB=modelB,\n",
    "    wandb_tag=wandb_tag,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_tag = f\"ot_model_fusion-{runA}-{runB}\"\n",
    "\n",
    "ot_fused_model, modelA_aligned = otfusion_experiment.run_otfusion(\n",
    "    batch_size=batch_size,\n",
    "    datamodule_type=datamodule_type,\n",
    "    datamodule_hparams=datamodule_hparams,\n",
    "    model_type=model_type,\n",
    "    model_hparams=model_hparams,\n",
    "    modelA=modelA,\n",
    "    modelB=modelB,\n",
    "    wandb_tag=wandb_tag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMC Barrier after alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmc_experiment.run_lmc(\n",
    "    datamodule_type=datamodule_type,\n",
    "    modelA=modelA_aligned,\n",
    "    modelB=modelB,\n",
    "    granularity=21\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss of the averaged models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule_hparams_lmc = {'batch_size': 1024, 'data_dir': BASE_DATA_DIR}\n",
    "datamodule_lmc = datamodule_type.get_data_module(**datamodule_hparams)\n",
    "datamodule_lmc.prepare_data()\n",
    "datamodule_lmc.setup('fit')\n",
    "\n",
    "vanilla_loss = lmc_utils.compute_loss(vanilla_averaging_model, datamodule_lmc)\n",
    "fused_loss = lmc_utils.compute_loss(ot_fused_model, datamodule_lmc)\n",
    "\n",
    "print(f\"Vanilla loss pre fine-tuning: {vanilla_loss}\")\n",
    "print(f\"Fused loss pre fine-tuning: {fused_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning the OT fused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_finetuning:\n",
    "    min_epochs = 50\n",
    "    max_epochs = 100\n",
    "    datamodule_hparams['batch_size'] = finetuning_batch_size\n",
    "    datamodule_hparams['data_augmentation'] = True\n",
    "\n",
    "    datamodule = datamodule_type.get_data_module(**datamodule_hparams)\n",
    "    lightning_params = {'optimizer': 'sgd', 'lr': otf_finetuning_lr, 'momentum': otf_finetuning_momentum, 'weight_decay': 0.0001, 'lr_scheduler': 'plateau', 'lr_decay_factor': 0.5, 'lr_monitor_metric': 'val_loss'}\n",
    "    otfused_lit_model = BaseModel(model_type=model_type, model_hparams=model_hparams, model=copy.deepcopy(ot_fused_model.model), **lightning_params)\n",
    "\n",
    "\n",
    "    logger_config = {'model_hparams': model_hparams} | {'datamodule_hparams': datamodule_hparams} | {'lightning_params': lightning_params} | {'min_epochs': min_epochs, 'max_epochs': max_epochs, 'model_type': model_type, 'datamodule_type': datamodule_type, 'early_stopping': True}\n",
    "    logger = get_wandb_logger(\"otfusion finetuning\", logger_config, [])\n",
    "    callbacks = []\n",
    "    monitor = 'val_loss'\n",
    "    patience = 20\n",
    "    callbacks.append(EarlyStopping(monitor=monitor, patience=patience))\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\")\n",
    "    callbacks.append(checkpoint_callback)\n",
    "    trainer = lightning.Trainer(min_epochs=min_epochs, max_epochs=max_epochs, logger=logger, callbacks=callbacks, deterministic='warn')\n",
    "\n",
    "\n",
    "    datamodule.prepare_data()\n",
    "\n",
    "    datamodule.setup('fit')\n",
    "\n",
    "    trainer.fit(otfused_lit_model, train_dataloaders=datamodule.train_dataloader(), val_dataloaders=datamodule.val_dataloader())\n",
    "\n",
    "\n",
    "    datamodule.setup('test')\n",
    "    trainer.test(otfused_lit_model, dataloaders=datamodule.test_dataloader())\n",
    "\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning the vanilla averaging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_finetuning:\n",
    "    min_epochs = 50\n",
    "    max_epochs = 100\n",
    "    datamodule_hparams['batch_size'] = finetuning_batch_size\n",
    "    datamodule_hparams['data_augmentation']=True\n",
    "\n",
    "    datamodule = datamodule_type.get_data_module(**datamodule_hparams)\n",
    "    lightning_params = {'optimizer': 'sgd', 'lr': vanilla_finetuning_lr, 'momentum': vanilla_finetuning_momentum, 'weight_decay': 0.0001, 'lr_scheduler': 'plateau', 'lr_decay_factor': 0.5, 'lr_monitor_metric': 'val_loss'}\n",
    "\n",
    "    vanilla_averaged_lit_model = BaseModel(model_type=model_type, model_hparams=model_hparams, model=copy.deepcopy(vanilla_averaging_model.model), **lightning_params)\n",
    "\n",
    "    logger_config = {'model_hparams': model_hparams} | {'datamodule_hparams': datamodule_hparams} | {'lightning_params': lightning_params} | {'min_epochs': min_epochs, 'max_epochs': max_epochs, 'model_type': model_type, 'datamodule_type': datamodule_type, 'early_stopping': True}\n",
    "    logger = get_wandb_logger(\"vanilla finetuning\", logger_config, [])\n",
    "    callbacks = []\n",
    "    monitor = 'val_loss'\n",
    "    patience = 20\n",
    "    callbacks.append(EarlyStopping(monitor=monitor, patience=patience))\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\")\n",
    "    callbacks.append(checkpoint_callback)\n",
    "    trainer = lightning.Trainer(min_epochs=min_epochs, max_epochs=max_epochs, logger=logger, callbacks=callbacks, deterministic='warn')\n",
    "\n",
    "\n",
    "    datamodule.prepare_data()\n",
    "\n",
    "    datamodule.setup('fit')\n",
    "\n",
    "\n",
    "    trainer.fit(vanilla_averaged_lit_model, train_dataloaders=datamodule.train_dataloader(), val_dataloaders=datamodule.val_dataloader())\n",
    "\n",
    "    datamodule.setup('test')\n",
    "\n",
    "    trainer.test(vanilla_averaged_lit_model, dataloaders=datamodule.test_dataloader())\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Finetuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OT Fused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ot_fused_model_finetuning_run is not None:\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f'model-fusion/Model Fusion/{ot_fused_model_finetuning_run}')\n",
    "\n",
    "    print(run.config)\n",
    "\n",
    "    batch_size = run.config['datamodule_hparams'].get('batch_size')\n",
    "\n",
    "    datamodule_type_str = run.config['datamodule_type'].split('.')[1].lower()\n",
    "    datamodule_type = DataModuleType(datamodule_type_str)\n",
    "    datamodule_hparams = run.config['datamodule_hparams']\n",
    "    datamodule_hparams['data_augmentation'] = False\n",
    "\n",
    "    model_type_str = run.config['model_type'].split('.')[1].lower()\n",
    "    model_type = ModelType(model_type_str)\n",
    "\n",
    "    model_hparams = run.config['model_hparams']\n",
    "\n",
    "    print(datamodule_hparams)\n",
    "    print(model_hparams)\n",
    "\n",
    "    checkpointFT = f'model-fusion/Model Fusion/model-{ot_fused_model_finetuning_run}:best_k'\n",
    "\n",
    "\n",
    "    run = wandb.init()\n",
    "\n",
    "    artifact = run.use_artifact(checkpointFT, type='model')\n",
    "    artifact_dir = artifact.download(root=CHECKPOINT_DIR)\n",
    "    otfused_lit_model = BaseModel.load_from_checkpoint(Path(artifact_dir)/\"model.ckpt\")\n",
    "    wandb_tags = [f\"{model_type.value}\", f\"{datamodule_type.value}\"]\n",
    "\n",
    "    datamodule, trainer = setup_testing(f'eval finetuning {ot_fused_model_finetuning_run}', model_type, model_hparams, datamodule_type, datamodule_hparams, wandb_tags)\n",
    "\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup('test')\n",
    "\n",
    "    trainer.test(otfused_lit_model, dataloaders=datamodule.test_dataloader())\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    finetuned_loss = lmc_utils.compute_loss(otfused_lit_model, datamodule_lmc)\n",
    "\n",
    "    print(f\"Finetuned otfused loss: {finetuned_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Averaging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vanilla_averaging_model_finetuning_run is not None:\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f'model-fusion/Model Fusion/{vanilla_averaging_model_finetuning_run}')\n",
    "\n",
    "    print(run.config)\n",
    "\n",
    "    batch_size = run.config['datamodule_hparams'].get('batch_size')\n",
    "\n",
    "    datamodule_type_str = run.config['datamodule_type'].split('.')[1].lower()\n",
    "    datamodule_type = DataModuleType(datamodule_type_str)\n",
    "    datamodule_hparams = run.config['datamodule_hparams']\n",
    "    datamodule_hparams['data_augmentation'] = False\n",
    "\n",
    "    model_type_str = run.config['model_type'].split('.')[1].lower()\n",
    "    model_type = ModelType(model_type_str)\n",
    "\n",
    "    model_hparams = run.config['model_hparams']\n",
    "\n",
    "    print(datamodule_hparams)\n",
    "    print(model_hparams)\n",
    "\n",
    "    checkpointFT = f'model-fusion/Model Fusion/model-{vanilla_averaging_model_finetuning_run}:best_k'\n",
    "\n",
    "\n",
    "    run = wandb.init()\n",
    "\n",
    "    artifact = run.use_artifact(checkpointFT, type='model')\n",
    "    artifact_dir = artifact.download(root=CHECKPOINT_DIR)\n",
    "    vanilla_averaged_lit_model = BaseModel.load_from_checkpoint(Path(artifact_dir)/\"model.ckpt\")\n",
    "    wandb_tags = [f\"{model_type.value}\", f\"{datamodule_type.value}\"]\n",
    "\n",
    "    datamodule, trainer = setup_testing(f'eval finetuning {vanilla_averaging_model_finetuning_run}', model_type, model_hparams, datamodule_type, datamodule_hparams, wandb_tags)\n",
    "\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup('test')\n",
    "\n",
    "    trainer.test(vanilla_averaged_lit_model, dataloaders=datamodule.test_dataloader())\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    finetuned_loss = lmc_utils.compute_loss(vanilla_averaged_lit_model, datamodule_lmc)\n",
    "\n",
    "    print(f\"Finetuned vanilla loss: {finetuned_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvature Analysis\n",
    "\n",
    "We use Pyhessian to compute sharpness and eigenspectrum of the base models, vanilla avg., OT fusion and the finetuned solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------- Computing sharpness -------\")\n",
    "\n",
    "print(\"------- Model A -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=modelA, compute_density=False, figure_name='modelA.pdf')\n",
    "\n",
    "print(\"------- Model B -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=modelB, compute_density=False, figure_name='modelB.pdf')\n",
    "\n",
    "print(\"------- Model A aligned to B -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=modelA_aligned, compute_density=False, figure_name='modelA_aligned.pdf')\n",
    "\n",
    "print(\"------- Vanilla avg model -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=vanilla_averaging_model, compute_density=False, figure_name='vanilla_avg.pdf')\n",
    "\n",
    "print(\"------- OT fusion model -------\")\n",
    "hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=ot_fused_model, compute_density=True, figure_name='otmodel.pdf')\n",
    "\n",
    "if vanilla_averaging_model_finetuning_run is not None:\n",
    "    print(\"------- Vanilla avg model (finetuned) -------\")\n",
    "    hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=vanilla_averaged_lit_model, compute_density=False, figure_name='vanilla_avg_ft.pdf')\n",
    "\n",
    "if ot_fused_model_finetuning_run is not None:\n",
    "    print(\"------- OT fusion model (finetuned) -------\")\n",
    "    hessian_comp = pyhessian_experiment.run_pyhessian(datamodule_type=datamodule_type, model=otfused_lit_model, compute_density=False, figure_name='otmodel_ft.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
